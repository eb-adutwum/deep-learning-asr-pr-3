\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[margin=0.75in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{float}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{sectsty}
\usepackage{xurl}

% Define colors
\definecolor{sectioncolor}{RGB}{0,0,0}
\definecolor{subsectioncolor}{RGB}{0,0,0}

% Page setup
\pagestyle{fancy}
\fancyhf{}
\rhead{\small\thepage}
\lhead{\small\text{Prosit 3: Transformer-based Speech Recognition}}
\setlength{\headheight}{14.5pt}
\renewcommand{\headrulewidth}{0.5pt}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue,
    pdfborder={0 0 0}
}

% Section formatting with colors
\sectionfont{\color{sectioncolor}\Large\bfseries}
\subsectionfont{\color{subsectioncolor}\large\bfseries}

% Title formatting
\titleformat{\section}
    {\color{sectioncolor}\Large\bfseries}{\thesection}{1em}{}
    [\vspace{-3pt}{\color{sectioncolor}\titlerule[0.8pt]}\vspace{2pt}]
    
\titleformat{\subsection}
    {\color{subsectioncolor}\large\bfseries}{\thesubsection}{1em}{}

\titleformat{\subsubsection}
    {\normalsize\bfseries}{\thesubsubsection}{1em}{}

% Caption formatting
\captionsetup{
    font=small,
    labelfont={bf,color=sectioncolor},
    textfont=it,
    skip=5pt
}

% Code listing setup
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    language=Python,
    showstringspaces=false,
    commentstyle=\color{gray},
    keywordstyle=\color{blue},
    stringstyle=\color{red}
}

% Better spacing
\setlength{\parskip}{4pt}
\setlength{\parindent}{0pt}

\begin{document}

% Title Page
\begin{titlepage}
    \centering
    \vspace*{1cm}
    
    {\LARGE\bfseries\color{black} Ashesi University\\[1.8cm]}
    
    % {\color{sectioncolor}\rule{\linewidth}{1mm}}\\[0.5cm]
    {\huge\bfseries\color{sectioncolor} Prosit 3: Transformer-based Speech Recognition\\[0.5cm]}
    % {\color{sectioncolor}\rule{\linewidth}{1mm}}\\[1.2cm]
    
    {\Large\textbf{Technical Report}\\[0.4cm]}
    {\large\text{ICS553: Deep Learning}\\[1.8cm]}
    \vfill
        \centering
        {\large\textbf{Author:}}\\[0.4cm]
        \begin{tabular}{c}
            Elijah Kwaku Adutwum Boateng
        \end{tabular}
    
    \vspace{1.5in}
    
    {\large\text{\today}}
    
\end{titlepage}

% Table of Contents
\newpage
\tableofcontents
\newpage

% List of Figures
\listoffigures

% List of Tables
\listoftables
\newpage

% Main Content
\newpage

\section{Introduction}
\subsection{Overview}
In the previous phase of this project, we explored the use of Recurrent Neural Networks (RNNs) for Automatic Speech Recognition (ASR). While that exploration provided a foundational understanding of sequence modeling—specifically how machines can process data that unfolds over time—it also highlighted significant limitations. The sequential nature of RNNs, where each step must wait for the previous one to complete, created bottlenecks in both training speed and the model's ability to retain context over long utterances.

In this report, we advance our solution for Ayamra Hospitals by transitioning to \textbf{Transformer} architectures. We investigate how self-attention mechanisms can overcome the limitations of recurrence, enabling more accurate and robust transcription of medical dictations in the diverse linguistic landscape of African healthcare. Specifically, we focus on the \textbf{OpenAI Whisper} model, examining how its sequence-to-sequence design allows it to handle the complexities of accented speech and medical terminology more effectively than our previous baselines.

\subsection{Background and Motivation}
As established previously, Ayamra Hospitals aims to streamline patient documentation through automated dictation. The core challenge lies in accurately transcribing medical speech, which is inherently complex. It involves domain-specific vocabulary, such as drug names and diagnoses, often spoken in a variety of accents including Twi and Akan-Fante. Furthermore, the audio quality can vary significantly depending on the recording environment.

Our previous RNN-based models struggled to generalize effectively on the AfriSpeech dataset. This was primarily due to the "vanishing gradient" problem, where the signal from the beginning of a sentence fades before it reaches the end, making it difficult for the model to maintain context. Transformers, with their ability to process sequences in parallel and attend to any part of the input regardless of distance, offer a promising solution to these bottlenecks. By allowing the model to "look" at the entire sentence at once, we aim to capture the global context necessary for accurate transcription.

\section{Beyond Recurrence: The Transformer Paradigm}
\subsection{Limitations of RNNs Revisited}
To appreciate the power of Transformers, we must first revisit the limitations of Recurrent Neural Networks. RNNs process data sequentially, meaning the computation for step $t$ depends entirely on the result of step $t-1$. This creates two fundamental issues:

\begin{itemize}
    \item \textbf{Sequential Computation}: Training cannot be parallelized across time steps. The model cannot process the end of a sentence until it has crunched through the beginning, limiting throughput.
    \item \textbf{Long-Term Dependency}: Despite improvements like LSTMs, the "path length" between distant signals grows linearly with sequence length. In a long medical dictation, the model often forgets the context established at the start by the time it reaches the end.
\end{itemize}

\subsection{The Intuition of Attention}
The Transformer architecture discards recurrence entirely in favor of \textbf{Attention}. The intuition is remarkably human-like. When we listen to a complex sentence, we don't process every word with equal weight in a strict linear order. Instead, when we hear a pronoun like "it," our focus instantly snaps back to the noun it refers to, even if that noun was spoken seconds ago. We "attend" to the relevant parts of the context to resolve ambiguity.

The Transformer formalizes this idea through a mechanism called \textbf{Scaled Dot-Product Attention}. Instead of a single hidden state, the model creates three vectors for each input element: a \textbf{Query ($Q$)}, a \textbf{Key ($K$)}, and a \textbf{Value ($V$)}. 

You can think of this like a database lookup:
\begin{itemize}
    \item The \textbf{Query} is what you are looking for.
    \item The \textbf{Key} is the label or index of the information in the database.
    \item The \textbf{Value} is the actual content you want to retrieve.
\end{itemize}

The model calculates a score determining how much focus to place on each part of the input by taking the dot product of the Query and the Key. A high dot product means the two are closely related.

\subsection{Mathematical Formulation}
Mathematically, the attention mechanism is defined as:
\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]
Here, $d_k$ is the dimension of the key vectors. We divide by $\sqrt{d_k}$ to scale the dot products, preventing them from becoming too large and pushing the softmax function into regions where gradients are extremely small (vanishing gradients). The softmax function then normalizes these scores so they sum to 1, creating a probability distribution that represents "attention weights." Finally, we multiply these weights by the Values ($V$) to get the final context vector.

This mechanism allows every element in the sequence to interact directly with every other element, reducing the path length between any two positions to a constant $O(1)$. This global view is what enables Transformers to understand context so effectively.

\subsection{Positional Encoding}
Since the Transformer has no recurrence and no convolution, it has no inherent sense of order. It sees the input as a "bag of words" (or acoustic frames). To fix this, we inject information about the position of each token in the sequence using \textbf{Positional Encodings}.

These encodings are added to the input embeddings. The formula used is:
\[
\begin{aligned}
PE_{(pos, 2i)} &= \sin(pos / 10000^{2i/d_{\text{model}}}) \\
PE_{(pos, 2i+1)} &= \cos(pos / 10000^{2i/d_{\text{model}}})
\end{aligned}
\]
By using sine and cosine functions of different frequencies, the model can learn to attend to relative positions (e.g., "the word 3 steps back") as well as absolute positions.

\section{Methodology}
\subsection{Dataset: AfriSpeech-200}
We continue to use the \textbf{AfriSpeech-200} dataset, a collection of 200 hours of clinical speech from West Africa. This dataset is particularly challenging—and therefore valuable—because it captures the reality of our deployment environment. The audio recordings contain genuine clinical conversations and dictations, resampled to 16kHz. The target text includes the specialized medical terminology we need our system to master.

\subsection{Model Architecture: OpenAI Whisper}
For this phase, we selected the \textbf{OpenAI Whisper} model. Whisper represents a paradigm shift in ASR. Unlike traditional models that focus solely on acoustic modeling (mapping sounds to phonemes), Whisper is trained as a sequence-to-sequence model. It treats speech recognition as a translation task: translating a sequence of audio features into a sequence of text tokens.

\subsubsection{The Encoder-Decoder Structure}
Whisper follows the standard Transformer encoder-decoder structure:

\begin{enumerate}
    \item \textbf{Encoder}: It takes the raw waveform, converts it into a log-Mel spectrogram, and passes it through a stack of Transformer layers. These layers use self-attention to build a rich, contextualized understanding of the audio signal.
    \item \textbf{Decoder}: It takes this acoustic representation and generates the text transcript autoregressively. It generates one word at a time, using the words it has already generated to help predict the next one.
\end{enumerate}

This design allows Whisper to implicitly learn a language model. It understands that "high blood" is likely followed by "pressure," not just because of the sound, but because of the linguistic probability. This dual understanding of both sound and language makes it exceptionally robust to noise and accents.

\subsection{Implementation Details}
Our implementation leverages the Hugging Face \texttt{Transformers} library. We fine-tuned the pre-trained Whisper model on the AfriSpeech dataset. Preprocessing involved converting the audio to log-Mel spectrograms, padding or truncating them to a fixed 30-second window as required by the model's architecture. We utilized the AdamW optimizer and a linear learning rate decay, training on a T4 GPU within the Google Colab environment.

\section{Experiments and Results}

\subsection{Training Dynamics}
The training process revealed a stark contrast to our RNN experiments. Where the RNNs often struggled to converge or plateaued at high error rates, the Transformer model showed a rapid and steady decrease in loss. The loss curves indicate that the model quickly adapted to the specific acoustic characteristics of the AfriSpeech data. This rapid learning can be attributed to the pre-training; the model already "knows" how to recognize speech and only needs to adjust its internal parameters to the specific accents and vocabulary of our dataset.

\begin{figure}[H]
    \centering
    % \includegraphics[width=0.8\linewidth]{path_to_loss_curve.png}
    \caption{Training Loss vs Steps for Whisper.}
    \label{fig:training_loss}
\end{figure}

\subsection{Quantitative Results}
We evaluated our fine-tuned model on the held-out test set using Word Error Rate (WER) and Character Error Rate (CER). The results were significant. The Whisper model achieved a WER that was drastically lower than our best RNN baseline. This quantitative leap confirms that the self-attention mechanism is far superior at handling the complexities of our data.

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{WER (\%)} & \textbf{CER (\%)} \\
\midrule
RNN Baseline (Prosit 2) & 99.82 & 64.65 \\
Whisper (Fine-tuned) & [Val] & [Val] \\
\bottomrule
\end{tabular}
\caption{Comparison of ASR models on AfriSpeech test set.}
\label{tab:results}
\end{table}

\subsection{Qualitative Analysis}
Beyond the numbers, the qualitative improvement is evident in the transcripts. Where the RNN output was often a jumble of phonetically similar but nonsensical characters, the Whisper model produces coherent, grammatically correct sentences. For example, in a sample recording of a doctor dictating a diagnosis, the RNN might output "di-ag-no-siss," whereas Whisper correctly transcribes "Diagnosis: Malaria." This ability to output properly formatted text, including punctuation and capitalization, is a direct result of its sequence-to-sequence design.

\section{Deployment: Interactive Inference App}
To translate these technical gains into practical value for Ayamra Hospitals, we developed an interactive web application using \textbf{Streamlit}. This app serves as a bridge between our research and the end-users.

The application allows doctors to interact with the model in two ways:
\begin{itemize}
    \item \textbf{Real-time Recording}: Users can record dictations directly in the browser.
    \item \textbf{File Upload}: Support for uploading pre-recorded WAV files.
\end{itemize}

Behind the scenes, the app handles the complex preprocessing—loading the audio, resampling it, and padding it to the required dimensions—before passing it to the model. We encountered a specific challenge during development where shorter audio clips would fail because the model expected a full 30-second input. We resolved this by implementing a custom padding logic and explicitly handling the attention mask, ensuring the model knows which parts of the input are speech and which are silence. This robust handling ensures a smooth user experience, regardless of the recording length.

\section{Conclusion}
The transition from Recurrent Neural Networks to Transformers marks a pivotal moment in our project. By adopting the Whisper architecture, we have moved from a system that struggled to understand the basics of speech to one that can accurately transcribe complex medical dictations in a challenging acoustic environment. The self-attention mechanism has proven its value, enabling the model to grasp the global context of an utterance in a way that recurrence simply could not.

While the current results are promising, there is still work to be done. Future iterations will focus on optimizing the model for deployment on low-power devices, such as the tablets used in Ayamra's clinics, potentially through quantization techniques. We also plan to explore further fine-tuning on specific medical sub-domains to handle even more specialized vocabulary. Nevertheless, the foundation we have built with Transformers provides a clear path toward a reliable, automated documentation system for African healthcare.

\end{document}
