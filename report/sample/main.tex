\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[margin=0.75in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{float}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{sectsty}
\usepackage{xurl}

% Define colors
\definecolor{sectioncolor}{RGB}{0,0,0}
\definecolor{subsectioncolor}{RGB}{0,0,0}

% Page setup
\pagestyle{fancy}
\fancyhf{}
\rhead{\small\thepage}
\lhead{\small\text{Prosit 2: Supervised Speech Recognition}}
\setlength{\headheight}{14.5pt}
\renewcommand{\headrulewidth}{0.5pt}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue,
    pdfborder={0 0 0}
}

% Section formatting with colors
\sectionfont{\color{sectioncolor}\Large\bfseries}
\subsectionfont{\color{subsectioncolor}\large\bfseries}

% Title formatting
\titleformat{\section}
    {\color{sectioncolor}\Large\bfseries}{\thesection}{1em}{}
    [\vspace{-3pt}{\color{sectioncolor}\titlerule[0.8pt]}\vspace{2pt}]
    
\titleformat{\subsection}
    {\color{subsectioncolor}\large\bfseries}{\thesubsection}{1em}{}

\titleformat{\subsubsection}
    {\normalsize\bfseries}{\thesubsubsection}{1em}{}

% Caption formatting
\captionsetup{
    font=small,
    labelfont={bf,color=sectioncolor},
    textfont=it,
    skip=5pt
}

% Code listing setup
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    language=SQL,
    showstringspaces=false,
    commentstyle=\color{gray},
    keywordstyle=\color{blue},
    stringstyle=\color{red}
}

% Better spacing
\setlength{\parskip}{4pt}
\setlength{\parindent}{0pt}

\begin{document}

% Title Page
\begin{titlepage}
    \centering
    \vspace*{1cm}
    
    {\LARGE\bfseries\color{black} Ashesi University\\[1.8cm]}
    
    % {\color{sectioncolor}\rule{\linewidth}{1mm}}\\[0.5cm]
    {\huge\bfseries\color{sectioncolor} Prosit 2: Supervised Speech Recognition\\[0.5cm]}
    % {\color{sectioncolor}\rule{\linewidth}{1mm}}\\[1.2cm]
    
    {\Large\textbf{Technical Report}\\[0.4cm]}
    {\large\text{ICS553: Deep Learning}\\[1.8cm]}
    \vfill
        \centering
        {\large\textbf{Author:}}\\[0.4cm]
        \begin{tabular}{c}
            Elijah Kwaku Adutwum Boateng
        \end{tabular}
    
    \vspace{1.5in}
    
    {\large\text{\today}}
    
\end{titlepage}

% Abstract
%\begin{abstract}
%\noindent


%\vspace{0.5cm}
%\noindent\textbf{Keywords:}  
%\end{abstract}

% Table of Contents
\newpage
\tableofcontents
\newpage

% List of Figures
\listoffigures

% List of Tables
\listoftables
\newpage

% Main Content
\newpage
% \section*{GitHub Repository}
% \noindent Link to Public Repository

\section{Introduction}
\subsection{Overview}
Over the past three weeks, I have been working on a tool that can automatically convert spoken words into written text, a process known as speech recognition. The idea is simple but quite useful: instead of typing or writing everything, a person can simply speak while the tool listens and turns those spoken words into text.

In this report, I will walk through why this project matters, the reasoning behind the solution approach, how the system was built and trained, and how well it performs. I will also highlight what worked well, what challenges came up along the way, and what can be improved in future iterations.

\subsubsection{Background of the Problem}
Ayamra is a network of hospitals across several major African cities. In many of these facilities, medical staff spend considerable time writing or typing patient notes, which slows documentation and reduces time for patient care.

To address this, Ayamra plans to introduce a system that allows doctors to dictate their notes and have them automatically transcribed into structured text within the hospital’s records. The goal is to save time and improve efficiency.

\subsubsection{The Task of Speech Recognition}
For humans, understanding speech feels almost too easy. We listen, connect sounds to words, and make sense of what is being said without much thought. For a computer, however, speech is not words; it is sound. Each sound or spoken word is a continuous wave made up of different frequencies and intensities that change over time.

When we record these waves, the computer does not see letters or words, it only sees numbers. The smooth waves are broken down into hundreds or thousands of small measurements that describe how strong each frequency is at every moment. A “moment” here refers to a very short slice of time, just a few milliseconds long during which the sound’s energy is measured. Together, these measurements form a long sequence of numbers, or sometimes a matrix, that represents the entire sound. In this form, every spoken sentence becomes a pattern of numbers that captures how the sound moves and changes through time.

The task, then, is to make sense of these patterns. If two people say the same word, their sound waves will not be identical, but they will still share certain features. By studying many examples, the computer can begin to notice which patterns of numbers tend to appear whenever a specific word is spoken. Over time, it learns to associate certain shapes and rhythms in the data with the words or phonemes\footnote{Any of the perceptually distinct units of sound in a specified language that distinguish one word from another.} they represent.

\subsection{Guiding Questions}
This project began with a few guiding questions that shaped how the work was approached and understood:

\begin{itemize}
    \item How can a computer learn to recognize speech as naturally as humans do?
    \item Why are recurrent neural networks a good fit for handling speech, and how do they make use of context over time?
    \item What does speech look like in a form that a model can understand, and how should it be processed before training?
    \item What challenges arise when training and evaluating such systems, and what lessons can be learned for future improvements?
\end{itemize}

These questions guided both the development of the model and the flow of this report. The sections that follow aim to answer them step by step—from understanding how machines process sequential data to how those ideas are applied to real speech.

\section{Beyond Fixed Data}
In this section, I explore how sequential data differs from fixed data, why order and context matter, and why traditional neural networks struggle to capture these relationships. These ideas lay the foundation for discussing recurrent neural networks later in this report.

\subsection{Fixed vs. Sequential Data}
Understanding speech begins with understanding the kind of data it is. Unlike images or tables, which come in fixed shapes and sizes, speech unfolds over time. The length of a recording depends on how long someone speaks, and every moment can carry subtle changes in tone, pace, and pronunciation. This makes speech sequential data, thus, data whose meaning depends not just on what is present, but also on the order in which it appears. In fixed data however, each piece of information stands on its own. The challenge of model on fixed data is to develop mapping between inputs and outputs while for sequential data, the model tries to capture relationships and dependencies between inputs across time.

\begin{table}[H]
    \centering
    \label{tab:fixed_vs_sequential}
    \begin{tabular}{|l|l|l|}
        \hline
        \textbf{Property} & \textbf{Fixed Data} & \textbf{Sequential Data} \\
        \hline
        Structure & Constant shape or size & Variable length and timing \\
        \hline
        Examples & Images and tables & Speech and video \\
        \hline
        Order importance & Low & High, as order defines meaning \\
        \hline
    \end{tabular}
    \caption{Comparison of Fixed and Sequential Data}
\end{table}

\subsection{Limitations of Traditional Neural Networks}
After understanding that order and context matter in speech, it becomes clear why traditional neural networks struggle with this kind of data. Their design is such that all inputs are treated as independent pieces of information. Each layer looks at the input as a fixed set of values, processes them once, and passes the result forward (in one direction). In doing so, the network captures patterns that exist within the data itself but ignores how inputs patterns change over time or are related to each other.

This design works well for fixed data, such as images or tables, where relationships are spatial rather than temporal. However, in speech, where meaning depends on the sequence of sounds, this independence becomes a limitation. The model cannot remember what came before or use that knowledge to interpret what comes next. Without any sense of history, it cannot capture how one sound influences the next or how words combine to form meaning.

\subsection{The Need for a Network that Remembers}
The limitations of traditional neural networks motivate the creation of a network that can remember. For data that changes over time, such as speech, the model must not only process each input but also keep track of what came before.

A network built for such data should be able to take inputs one at a time, in sequence. After processing each input, it should keep some form of memory that captures what it has learned so far. When the next input arrives, the network can then use both the new information and what it remembers from before to make sense of the current moment.

In this way, the network builds an internal understanding of how things unfold through time. This idea of a model that remembers is the foundation on which more advanced sequence-based networks are built.

\subsection{Modelling Sequences}
A notable point in our discussion is that not all sequence problems are the same. Depending on what kind of input and output we are dealing with, different structures can be used to describe how data moves through the model.

\subsubsection{One-to-Many (Sequence Generation)}
In some cases, a single input can generate a sequence of outputs. Imagine a system that takes a still image and produces a caption word by word, describing what it sees. The model starts from one input and unfolds a series of outputs over time. This setup is often called a \textit{sequence generation} problem because the goal is to produce a sequence from a single starting point.

\subsubsection{Many-to-One (Sequence Classification)}
Here, multiple inputs combine to produce a single output. A good example is sentiment analysis, where a model reads an entire sentence and predicts whether the emotion is positive or negative. The model must remember earlier words in order to interpret the sentence as a whole. This type of problem is called \textit{sequence classification}, since a full sequence is reduced to one label or outcome.

\subsubsection{Many-to-Many (Sequence-to-Sequence)}
Finally, there are problems where both the input and output are sequences. Machine translation is one example: a sequence of words in one language is mapped to a sequence of words in another. Speech recognition also falls into this category, where a sequence of sounds is converted into a sequence of written words. In such cases, the model must handle variable input and output lengths while maintaining the right alignment between them. This setup is often referred to as a sequence-to-sequence (seq-to-seq) problem.


\subsection{The Case of Ayamra: Choosing the Many-to-Many Approach}
For Ayamra’s dictation system, the goal is to turn a sequence of spoken sounds into a sequence of written words. The audio and text unfold over time, and their lengths rarely match.

This makes the problem a clear \textit{many-to-many} or \textit{sequence-to-sequence} task.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{ayamra-seq.png}
    \caption{An illustration of Ayamra’s speech-to-text system.}
    \label{fig:ayamra_seq_to_seq}
\end{figure}


\section{Theoretical Foundations}
Up to this point, we have seen why speech requires a model that can process information over time rather than all at once. Recurrent Neural Networks (RNNs) were designed for this purpose. They introduce a feedback connection that allows information from earlier steps to influence later ones, giving the network a sense of memory. This makes them naturally suited for sequence tasks like speech recognition. 

In this section, we explore the key ideas that make RNNs work—from the basics of acoustic and linguistic modelling to the mechanism of learning and the challenges that arise when dealing with long sequences.

% \subsection{Acoustic and Linguistic Modelling Concepts}
% Speech is more than just sound—it carries structure and meaning. To recognize spoken language, a model must learn two things at once: the acoustic patterns that make up speech sounds, and the linguistic patterns that form words and sentences. These two aspects are captured through \textit{acoustic modelling} and \textit{linguistic modelling}.

% \textbf{Acoustic modelling} focuses on the sound itself. It deals with the raw audio signal—the vibrations of air that change over time—and learns how different sounds correspond to different speech units such as phonemes or syllables. In practice, this often involves representing the sound as numerical features like Mel-Frequency Cepstral Coefficients (MFCCs) or spectrograms that describe how energy is distributed across frequencies.

% \textbf{Linguistic modelling}, on the other hand, helps the system make sense of what those sounds mean. It captures the statistical and grammatical patterns of language—how certain words tend to follow others and how context changes meaning. This part ensures that the model doesn’t just recognize sounds correctly but also arranges them into valid and meaningful sentences.

% Together, these two layers of understanding allow a speech recognition system to bridge the gap between sound and language. The acoustic model hears, while the linguistic model interprets.


\subsection{Introduction to Recurrent Neural Networks (RNNs)}
We remember that a neural network is a system of connected layers that learn to transform input data into an output through a series of mathematical operations. Each layer receives a set of inputs, multiplies them by weights, adds a bias, and applies a non–linear activation function. In essence, every neuron performs a simple calculation: $y = f(Wx + b)$; where \(x\) is the input, \(W\) the weight matrix, \(b\) the bias, and \(f(\cdot)\) an activation function. 

When stacked, these layers (input, hidden, and output) form a network that can approximate complex mappings between inputs and outputs. During training, the network adjusts its weights so that its predictions move closer to the desired outputs.

Recurrent Neural Networks (RNNs) extend this idea by introducing a feedback loop. Instead of simply moving the output of one layer forward to the next, the RNN also sends part of that output back into the same layer as input for the next timestep. This feedback creates a form of internal memory, allowing the network to retain what it has learned from previous steps. In doing so, RNNs can model sequences with awareness of both past and present information.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{trad-nn-rnn.png}
    \caption{Traditional feedforward neural network (left) and a recurrent neural network (right).}
    \label{fig:trad_nn_vs_rnn}
\end{figure}

\subsection{The Mechanism of Learning}
Training a recurrent network follows the same general idea as any neural network: make a prediction, compare it to the correct answer, and adjust the model so that next time, the prediction is closer to the truth. What makes RNNs special is that this process unfolds through time such that, each step influences the next.

\subsubsection{Forward Pass: With Recurrence}
During the forward pass, the RNN processes a sequence one element at a time. At each time step \(t\), it takes an input \(x_t\), combines it with the memory from the previous step \(h_{t-1}\), and produces both an updated hidden state and (optionally) an output:
\[
h_t = \phi(W_x x_t + W_h h_{t-1} + b_h), \qquad
y_t = \psi(W_y h_t + b_y).
\]
As this repeats across the sequence, the network builds a chain of dependencies, so that each hidden state depends on all previous inputs.

\subsubsection{Computing the Loss}
After the forward pass, the model’s predictions are compared with the true targets to measure how far off they are. This difference is captured by a \textit{loss function}, which quantifies this difference as the error. Over many training steps, learning simply means making this number smaller.

At the very core, what we are really doing is a form of classification. For every point in the sequence, the model tries to decide which symbol or word should come next based on what it has already seen. Since this happens repeatedly across many timesteps, we need a loss function that can efficiently measure how well the model is classifying these multiple outputs. The most common and effective choice for this is the \textbf{Cross-Entropy Loss (CEL)}.

\paragraph{Cross-Entropy Loss (CEL):} At a high level, cross-entropy measures the difference between two probability distributions, the model’s predicted probabilities and the true distribution (which is 1 for the correct class and 0 for the others).
\[
\mathcal{L}_{CE} = -\sum_{t=1}^T \log p(y_t^{\text{true}} \,|\, x_{1:t}),
\]
where \(p(y_t^{\text{true}})\) is the model’s predicted probability of the correct class at timestep \(t\). 
What the function is doing is taking the \textit{negative log-likelihood} of the correct class. It measures how improbable the model believes the true answer to be. For instance, if the model predicts probabilities \([0.7, 0.2, 0.1]\) for three classes and the correct class is the first, the loss becomes \(-\log(0.7)\), which is small. Conversely, if the model predicts \([0.1, 0.2, 0.7]\), the loss becomes \(-\log(0.1)\), which is much larger.


\paragraph{Connectionist Temporal Classification (CTC)}
In speech recognition, however, this neat one-to-one alignment between inputs and outputs breaks down. A short word like “cat” can take anywhere from 100 to 1000 milliseconds to say, and there is no simple way to know which exact audio frame corresponds to each letter. To handle this, we use an alternative called \textit{Connectionist Temporal Classification (CTC)}.

CTC is designed for sequence tasks where input and output lengths differ and alignment is unknown. Instead of forcing a direct match between frames and symbols, it allows the model to explore all possible alignments and sum over them. It introduces a special “blank” label, which acts as a placeholder for frames that do not correspond to any character, and collapses repeated labels to form the final transcription.  

\subsubsection{Backpropagation Through Time (BPTT)}
Once the loss is computed, the model must learn from it. Just like in standard networks, this is done through backpropagation by computing how each weight contributed to the error and adjusting it in the right direction. But in an RNN, the same weights are reused at every timestep, so errors must be traced backward not just through layers, but also through time. This process is called \textit{Backpropagation Through Time (BPTT)}.

During BPTT, the chain of RNN cells is “unrolled” over all timesteps. The gradient of the loss at the final timestep flows backward through each previous hidden state, adjusting how earlier steps influenced later predictions. In doing so, the model learns which earlier patterns are most useful for understanding later parts of the sequence. Conceptually, it’s like revisiting each memory and deciding how much credit or blame it deserves for the final outcome.

\subsubsection{Optimization}
The gradients from BPTT are then used to update the model’s parameters through an optimization algorithm such as stochastic gradient descent (SGD) or Adam. These optimizers adjust weights slightly after each batch of examples, slowly pushing the model toward lower error. Over time, the RNN improves its internal memory so that the hidden state encodes more meaningful context.

% \subsection{The Gradient Problem of RNNs}
% \subsection{RNN Variants}
% \subsubsection{Gated Recurrent Units(GRU)}
% \subsubsection{Long Short Term Memory (LSTM)}
% \subsubsection{A Note on Bidirectionality}
\subsection{The Gradient Problem of RNNs}
Recurrent networks reuse the same weights at every timestep. When learning, the gradient of the loss must flow backward through all these steps to update those shared weights. This is what allows an RNN to learn from earlier parts of a sequence. But this creates a unique challenge. Because each step depends on the previous one, the gradients get multiplied repeatedly by the same set of weights as they travel backward through time. Over many steps, these repeated multiplications can cause the gradients to either become extremely small or extremely large. These two extremes are known as the \textit{vanishing} and \textit{exploding} gradient problems.

Mathematically, this happens because at every timestep the gradient is multiplied by the derivative of the recurrent weights and activation functions. If these multipliers are typically less than one (say \(0.9\)), after 50 timesteps you have roughly \(0.9^{50}\), an extremely small number, almost zero. If they are slightly greater than one (say \(1.1\)), then \(1.1^{50}\) becomes enormous. As a result, when we compute the gradients during backpropagation, earlier timesteps either receive almost no update (they “forget”) or receive updates so large that learning becomes unstable.

\paragraph{Practical impact.}
Both problems limit how effectively an RNN can learn long-term dependencies. In the vanishing case, the model simply ignores distant history. In the exploding case, training becomes numerically unstable and may diverge entirely. Either way, the network’s ability to build meaningful temporal memory breaks down.

\paragraph{Common remedies:}
\begin{itemize}
    \item \textbf{Gradient clipping:} limit how large a gradient can become by rescaling it when it exceeds a threshold but this prevents explosions.
    \item \textbf{Careful initialization:} choose weight values (often orthogonal or identity-like) so that repeated multiplications neither shrink nor grow too fast.
    \item \textbf{Non-saturating activations:} functions like ReLU or LeakyReLU avoid the extreme squashing that leads to vanishing gradients.
    \item \textbf{Normalization:} applying layer or batch normalization helps maintain stable gradient scales across timesteps.
\end{itemize}


\subsection{RNN Variants}
The gradient problems discussed earlier motivated a new family of architectures that make it easier for information and the gradients that train it to travel across many timesteps. The core idea behind these \textit{gated RNNs} is to create \emph{learned, linear pathways} through time so that important signals can persist without being repeatedly squashed or amplified. These gates act like valves, deciding what to remember, what to forget, and when to let information influence the output.

\subsubsection{Long Short-Term Memory (LSTM)}
The \textit{Long Short-Term Memory} network introduces a separate internal memory called the \emph{cell state}, denoted \(c_t\), which runs through the network with only minor, gated interactions. Each LSTM cell uses three main gates: the input gate, forget gate, and output gate. These gates control how new information enters, how much of the old memory to keep, and what part of the cell state should affect the output.

\[
\begin{aligned}
i_t &= \sigma(W_i x_t + U_i h_{t-1} + b_i) &&\text{(input gate)}\\
f_t &= \sigma(W_f x_t + U_f h_{t-1} + b_f) &&\text{(forget gate)}\\
o_t &= \sigma(W_o x_t + U_o h_{t-1} + b_o) &&\text{(output gate)}\\
\tilde{c}_t &= \tanh(W_c x_t + U_c h_{t-1} + b_c) &&\text{(candidate state)}\\
c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t, \qquad
h_t = o_t \odot \tanh(c_t)
\end{aligned}
\]

\textbf{Why it helps.}  
The strength of the LSTM lies in the \emph{additive} update of the cell state \(c_t\). Instead of repeatedly multiplying gradients through many nonlinear transformations (which causes vanishing or exploding), the LSTM allows gradients to flow along this cell state nearly unchanged. The forget gate \(f_t\) controls how much past information to keep, while the input gate \(i_t\) determines how much new information to write. Together, they form a dynamic memory system that learns what is worth remembering and what can be safely discarded. When tuned well, this makes the network capable of capturing dependencies that span hundreds of timesteps.

\subsubsection{Gated Recurrent Unit (GRU)}
The \textit{Gated Recurrent Unit} simplifies the LSTM while preserving its key insight: control the flow of information through time. It merges the cell and hidden states into a single vector and uses only two gates, thus, the update gate and reset gate to manage memory.

\[
\begin{aligned}
z_t &= \sigma(W_z x_t + U_z h_{t-1} + b_z) &&\text{(update gate)}\\
r_t &= \sigma(W_r x_t + U_r h_{t-1} + b_r) &&\text{(reset gate)}\\
\tilde{h}_t &= \tanh(W_h x_t + U_h (r_t \odot h_{t-1}) + b_h),\\
h_t &= (1 - z_t)\odot h_{t-1} + z_t \odot \tilde{h}_t
\end{aligned}
\]

\textbf{Why it is useful:}  
The GRU’s update rule forms an \emph{adaptive shortcut} through time. The update gate \(z_t\) decides how much of the previous state should pass through unchanged versus how much should be replaced by the new candidate \(\tilde{h}_t\). When \(z_t\) is small, the old state flows almost directly to the next step, allowing long-term information to persist and gradients to pass through with minimal distortion. When \(z_t\) is large, the model refreshes its memory with new input. This balance between preservation and adaptation makes GRUs both computationally efficient and resistant to vanishing gradients.

\subsubsection{A Note on Bidirectionality}
Standard RNNs process data in one direction (from past to future) but many sequence tasks like speech and text, benefit from context on both sides. \textit{Bidirectional RNNs (BiRNNs)} solve this by running two separate RNNs: one forward and one backward. Their hidden states are then combined:
\[
\overrightarrow{h}_t = \mathrm{RNN}_f(x_{1:t}), \qquad
\overleftarrow{h}_t = \mathrm{RNN}_b(x_{T:t}), \qquad
h_t = [\overrightarrow{h}_t; \overleftarrow{h}_t].
\]
Although bidirectionality does not fix gradient issues directly, it enriches each timestep with both \emph{past and future} context. This makes patterns like word dependencies in speech easier to model.

\section{Applications of RNNs}
Before advancing to the main Automatic Speech Recognition (ASR) task using the AfriSpeech dataset, it was essential to first apply RNNs to more controlled sequence problems. Tasks such as text translation and speech recognition on the LibriSpeech dataset provided a structured pathway to experiment with sequence-to-sequence architectures, understand gradient behavior, and evaluate different recurrent variants under manageable settings.

\subsection{Text Translation with Fra-Eng}
\subsubsection{Overview of Task and Data}
The task involves translating French sentences into English using recurrent neural network architectures. The dataset used was the \textit{English–French Parallel Corpus} available on Kaggle\footnote{\url{https://www.kaggle.com/datasets/ilhansevval/eng-fra}}, containing thousands of aligned sentence pairs drawn from conversational and literary sources. Each sentence pair was normalized to lowercase, tokenized, and cleaned to remove non-alphabetic symbols. Rare words were replaced with the \texttt{<UNK>} token to limit the vocabulary to the 20,000 most frequent tokens. Sentences longer than 15 words were filtered out, resulting in approximately 130,000 sentence pairs. 

The dataset was divided into 70\% training, 20\% validation, and 10\% test sets. Both source and target sequences were padded to a common length and wrapped with special tokens: \texttt{<SOS>} and \texttt{<EOS>}. The preprocessing ensured that training batches had uniform dimensions suitable for recurrent processing.

\subsubsection{Model Implementation}
A sequence-to-sequence (\textit{Seq2Seq}) framework was employed with an encoder–decoder architecture trained using \textit{Cross-Entropy Loss}. 

In this setup, both the \textit{encoder} and \textit{decoder} are recurrent neural networks (RNNs) with their own parameters and hidden states. The encoder reads the entire input sentence word by word, updating its hidden state at each timestep to capture how words relate to one another over time. By the end of the sequence, it condenses all this information into a fixed-length hidden representation known as the \emph{context vector}, which serves as a summary of the sentence’s meaning. Intuitively, the encoder functions like a listener who processes and internalizes the full message before responding.

The decoder, also an RNN, takes this context vector and generates the translation one word at a time. At each step, it predicts the next word based on the context provided by the encoder and the sequence of words it has already produced. In essence, the decoder acts as the “speaker,” unfolding the compressed thought from the encoder into a coherent sentence in the target language. Together, these two RNNs form a sequence-to-sequence pipeline.

Several variants were tested, beginning with a unidirectional vanilla RNN and progressively replacing the recurrent units with GRU and LSTM cells. For each model, both unidirectional and bidirectional encoders were explored, though the latter consistently achieved better performance. 

Each model consisted of two recurrent layers with 512 hidden units and dropout regularization (0.3). The decoder predicted the next target word token by token, conditioned on its hidden state and the encoder’s context vector.

\subsubsection{Training and Optimization}
Training was conducted on GPU for 30 epochs with a batch size of 64, using the Adam optimizer (\(lr = 10^{-4}\)) and a weight decay of \(10^{-5}\). The loss function ignored padding indices to prevent artificial gradient contributions. Both training and validation losses were tracked, along with Word Error Rate (WER) and Character Error Rate (CER) metrics to evaluate translation quality.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{training_metric.png}
    \caption{Training and validation curves for loss, WER, and CER across epochs.}
\end{figure}
The figure should show steady convergence and declining error rates, reflecting improved translation accuracy.


\subsubsection{Evaluation and Results}
All evaluations were conducted on a \textit{held-out test set} to ensure that the reported performance reflects the model’s generalization ability rather than memorization of training data. The final results are summarized in Table~\ref{tab:wer_results}. Each model was evaluated using Word Error Rate (WER) on the test data. The bidirectional LSTM achieved the lowest WER, demonstrating the effectiveness of gated architectures in capturing long-term dependencies crucial for translation.

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Direction} & \textbf{WER (\%)} \\
\midrule
Vanilla RNN & Unidirectional & 56.61 \\
Vanilla RNN & Bidirectional & 50.24 \\
GRU & Bidirectional & 43.32 \\
LSTM & Bidirectional & \textbf{41.39} \\
\bottomrule
\end{tabular}
\caption{Comparison of Encoder–Decoder architectures on the French–English translation task (word-level vocabulary), evaluated on a held-out test set.}
\label{tab:wer_results}
\end{table}

\paragraph{Qualitative Results.}
To complement the quantitative metrics, qualitative inspection was performed on several samples from the held-out test set. Figure~\ref{fig:translation_samples} displays example French–English sentence pairs alongside the model’s predictions. The Bidirectional LSTM produced fluent and grammatically coherent translations for most short and medium-length sentences, though minor deviations appeared for longer or idiomatic expressions.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{translation_samples.png}
    \caption{Example translation outputs from the Bidirectional LSTM model on the held-out test set.}
    \label{fig:translation_samples}
\end{figure}

Each sample shows the French input, the human reference translation, and the model’s predicted English output.


% \subsection{Speech Recognition}
% \subsubsection{Overview of Task and Data}
% \subsubsection{Model Implementation}
% \subsubsection{Training and Optimization}
% \subsubsection{Evaluation and Results}
\subsection{Speech Recognition with LibriSpeech}
\subsubsection{Overview of Task and Data}
To extend the application of recurrent networks from text-to-text translation to speech, experiments were conducted on the \textit{LibriSpeech} corpus\footnote{\url{https://openslr.org/12/}}. The dataset consists of clean English speech sampled at 16 kHz. Initial experiments were performed using the \textit{dev-clean} subset, followed by training on the larger \textit{train-clean-100} partition to improve generalization.

Before processing, all audio was converted to mono to ensure a consistent single-channel representation. Stereo recordings typically contain two channels (a two-dimensional array) representing sound captured from the left and right microphones, while mono audio is a single channel (a one-dimensional array) that captures the same sound pressure variations over time. For speech recognition, the second channel offers no additional linguistic information because both channels carry the same spoken content with only minor differences. Converting to mono therefore reduces data dimensionality without losing meaning, preserving the essential amplitude changes that correspond to the spoken signal while simplifying computation.

Each audio file was then converted into a time–frequency representation using a 40-dimensional \textit{log-Mel spectrogram}. The Mel scale approximates how humans perceive sound frequencies\footnote{The Mel scale compresses higher frequencies and expands lower ones to align with the nonlinear sensitivity of human hearing.}. Instead of raw waveforms, which can contain tens of thousands of samples per second, this transformation captures how energy is distributed across frequency bands over short time windows (typically 25 ms). 

To enrich temporal context, first and second-order derivatives (\textit{delta} and \textit{double-delta} features) were computed and concatenated, forming a 120-dimensional feature vector per frame. These reflect not just what frequencies are present, but how they change. Each feature vector was then normalized to zero mean and unit variance to remove loudness bias and stabilize learning. 

The task was formulated as character-level transcription, mapping variable-length sequences to text sequences without explicit alignment. Because the lengths of speech and text differ, \textit{Connectionist Temporal Classification (CTC)} was used as a loss function to implicitly learn alignments during training.

\subsubsection{Model Implementation}
An \textit{Encoder–Fully Connected} (EncFC) architecture was employed, trained end-to-end using the CTC objective. The encoder, implemented as a recurrent neural network (RNN), processes each timestep of the feature sequence, learning how local `sound` patterns (like phonemes) evolve into longer linguistic units (like words). Over time, the hidden layers accumulate this information, forming a high-level internal representation of the entire utterance. 

The output of the encoder is then passed through a fully connected layer that converts these hidden representations into probability distributions over characters at each timestep. The CTC layer aligns these predictions with the correct transcriptions, allowing the model to learn even when no explicit frame-to-character mapping is given. 

Conceptually, the model listens to the audio frame by frame, builds an internal understanding of what it hears, and progressively decides which characters best describe that sound. The recurrent structure makes it sensitive to temporal order, while the CTC loss ensures it can handle sequences where some frames may correspond to silence or stretched pronunciations.

Several recurrent variants were tested:
\begin{itemize}
    \item A unidirectional vanilla RNN baseline,
    \item A bidirectional RNN to capture context from both past and future frames,
    \item Gated Recurrent Units (GRU), and
    \item Long Short-Term Memory (LSTM) networks.
\end{itemize}
All models used two recurrent layers with 512 hidden units and dropout regularization (0.3) to prevent overfitting.

\subsubsection{Training and Optimization}
Training was conducted on GPU for up to 30 epochs with a batch size of 32, using the Adam optimizer (\(lr = 10^{-4}\)) with gradient clipping to stabilize CTC training. The CTC loss automatically handled alignment between input frames and target transcriptions. 

To improve convergence stability, the training data was sorted by input sequence length, allowing the model to encounter shorter, simpler examples first. This curriculum-like strategy helped the network form more stable initial mappings between sound and text, as shorter utterances often consist of simpler word structures. While the improvement in final accuracy was not significant, the training process became more stable and consistent across epochs. This behavior mirrors how humans learn by mastering shorter or easier patterns before tackling longer, more complex ones. Exploring this idea further could yield more robust models.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{vrnn-tc.png}
    \caption{Training and validation curves for CTC loss, 30 epochs on the LibriSpeech dataset.}
\end{figure}
The figure should illustrate steady convergence and decreasing error rates, reflecting improved transcription accuracy over time.

\subsubsection{Evaluation and Results}
All evaluations were performed on a held-out test set from LibriSpeech. The results are summarized in Table~\ref{tab:librispeech_results}. Each model’s performance is reported in terms of Word Error Rate (WER) and Character Error Rate (CER)\footnote{WER and CER measure how closely the model’s predicted transcription matches the reference text. WER compares the number of word-level insertions, deletions, and substitutions needed to correct the prediction, while CER performs the same comparison at the character level. Intuitively, lower values indicate that fewer edits are needed—meaning the model’s output is closer to the ground truth.}. As with the translation experiments, bidirectional and gated architectures consistently outperformed the unidirectional RNN baseline. The LSTM model achieved the best overall results.


\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Direction} & \textbf{WER (\%)} & \textbf{CER (\%)} \\
\midrule
Vanilla RNN & Unidirectional & 80.71 & 33.68 \\
Vanilla RNN & Bidirectional & 80.00 & 33.33 \\
GRU & Bidirectional & 71.37 & 27.41 \\
LSTM & Bidirectional & \textbf{68.35} & \textbf{25.76} \\
\bottomrule
\end{tabular}
\caption{Comparison of recurrent architectures on the LibriSpeech \textit{train-clean-100} subset (30 epochs).}
\label{tab:librispeech_results}
\end{table}

Earlier baseline experiments on the \textit{dev-clean} subset showed higher error rates (unidirectional RNN: WER 99.28\%, CER 51.69\%; bidirectional RNN: WER 91.34\%, CER 46.23\%), but these improved significantly with the larger training data and longer training duration.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{libri_samples.png}
    \caption{Example qualitative outputs showing transcriptions from the best model on the LibriSpeech.}
    \label{fig:librispeech_samples}
\end{figure}

Overall, the results highlight the effectiveness of gated recurrent architectures, particularly LSTMs, for speech recognition tasks.


% \section{AfriSpeech Speech Recognition}
% \subsection{Overview of Task and Data}
% \subsection{Model Implementation}
% \subsection{Attention-Enhanced RNN (Extended Task)}
% \subsubsection{Intuition and Motivation for Attention}
% \subsubsection{Attention Mechanism}
% \subsection{Training and Optimization}
\section{AfriSpeech Speech Recognition}
\subsection{Overview of Task and Data}
Building on the earlier LibriSpeech experiments, the AfriSpeech dataset\footnote{\url{https://huggingface.co/datasets/intronhealth/afrispeech-200}} was used as the main testbed for developing a medical-domain Automatic Speech Recognition (ASR) system. The dataset contains audio recordings of clinical conversations and dictated notes collected from Ghanaian healthcare settings, introducing greater linguistic and acoustic variability compared to LibriSpeech. 

A key feature of AfriSpeech is its inclusion of regional language accents, primarily \textit{Akan-Fante} and \textit{Twi}, which contribute to diverse pronunciation patterns and phonetic variations in English speech. This makes the dataset particularly relevant for modeling real-world African medical communication, where code-switching and accent diversity are common. 

The dataset is organized into three main subsets: \texttt{train}, \texttt{dev}, and \texttt{test}. The \texttt{train} set was used for model fitting, while the \texttt{dev} split served as the validation set for hyperparameter tuning and monitoring convergence. The \texttt{test} split was reserved strictly for final evaluation to assess generalization on unseen speech. 

The preprocessing pipeline closely followed that used for LibriSpeech. All audio was converted to mono (a one-dimensional array of amplitude values over time) to ensure consistency and reduce redundant information from stereo channels (two-dimensional arrays). Each recording was then transformed into a log-Mel spectrogram with 40 Mel bands, followed by delta and double-delta derivatives to capture temporal changes in energy distribution. These were concatenated to form a 120-dimensional feature vector per frame and normalized to zero mean and unit variance. The task was formulated as character-level transcription and trained using the Connectionist Temporal Classification (CTC) objective to learn alignments automatically.


\subsection{Model Implementation}
Given the strong performance of the Encoder–Fully Connected (EncFC) CTC model on LibriSpeech, the same design was adopted for AfriSpeech. The encoder, implemented as a recurrent neural network (RNN), processed the enhanced acoustic feature sequences and passed them through a fully connected layer to generate frame-wise character probabilities. The CTC loss then handled alignment between variable-length audio and text sequences.

Although this architecture performed reliably on LibriSpeech, results on AfriSpeech were less satisfactory. To improve sequence alignment and better capture long-range dependencies, a standard Encoder–Decoder (EncDec) model with attention was attempted. However, due to the size and variability of the AfriSpeech recordings, these experiments encountered repeated out-of-memory (OOM) errors during training. As a result, only the baseline vanilla RNN–CTC model was successfully trained to completion. Attention-based models remain a promising next step for future iterations.

\subsection{Training and Optimization}
Training was conducted for up to 50 epochs with the Adam optimizer (\(lr = 10^{-4}\)), batch size 32, and gradient clipping. Data batches were sorted by input length to stabilize the CTC alignment process, allowing the model to encounter shorter and simpler examples first. This approach provided smoother learning in the early epochs—analogous to a curriculum learning effect—though it did not significantly improve final accuracy. Investigating this strategy further could yield better convergence in future work.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{afri-train.png}
    \caption{Training and validation curves for CTC loss and validation Word Error Rate (WER) on the AfriSpeech dataset over 50 epochs.}
    \label{fig:afrispeech_training}
\end{figure}

 The loss shows consistent convergence, though error rates remain high, reflecting the increased difficulty of the domain.

\subsection{Evaluation and Results}
Evaluation was performed on a held-out test set from AfriSpeech. The model achieved an average CTC loss of 2.2754, with a Word Error Rate (WER) of 99.82\% and Character Error Rate (CER) of 64.65\%. Despite reasonable convergence in loss, the predictions were largely unintelligible, indicating that the model captured low-level acoustic regularities but struggled to map them accurately to language.

To illustrate this behavior, Figure~\ref{fig:afrispeech_samples} presents sample outputs from the held-out test set, showing both the reference and the model’s predicted transcriptions. The results reveal a consistent pattern of phonetic drift and incoherent character sequences—a typical symptom of undertrained or domain-mismatched CTC systems.  

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{afri-samples.png}
    \caption{Sample outputs from the AfriSpeech held-out test set using the vanilla RNN–CTC model. Each sample shows the reference transcription (R) and the model’s prediction (H).}
    \label{fig:afrispeech_samples}
\end{figure}

Overall, while the model demonstrated stable training behavior, its generalization performance lagged due to data complexity, limited compute capacity, and the mismatch between LibriSpeech hyperparameters and AfriSpeech’s acoustic domain. Future work will focus on integrating attention mechanisms and optimizing memory efficiency to better model longer utterances and medical vocabulary.

% \section{Conclusion and Recommendations}
% \subsection{Summary of Findings}
% \subsection{Practical Recommendations for Ayamra Hospitals}
\section{Conclusion and Recommendations}
\subsection{Summary of Findings}
This work explored the application of recurrent neural networks (RNNs) to sequence modeling tasks, progressing systematically from text translation to speech recognition using the LibriSpeech and AfriSpeech datasets. Across all experiments, the study demonstrated that while vanilla RNNs can capture short-term dependencies, gated variants such as LSTMs and GRUs consistently achieve superior performance due to their ability to preserve gradients and model long-range temporal relationships. 

On the LibriSpeech dataset, the bidirectional LSTM achieved the lowest Word Error Rate (WER) and Character Error Rate (CER), validating the effectiveness of gated architectures for structured audio tasks. However, results on the AfriSpeech dataset revealed the practical challenges of deploying ASR models in low-resource, domain-specific contexts. Despite stable training convergence, the model struggled to generalize due to the acoustic variability and specialized medical vocabulary in the data. Experiments with attention mechanisms were attempted to improve alignment, but hardware memory limitations prevented full implementation. These findings underscore both the promise and the computational demands of modern RNN-based ASR systems in real-world African healthcare settings.

In conclusion, while the AfriSpeech model currently exhibits high error rates, the lessons learned from the LibriSpeech and translation experiments provide a clear roadmap toward robust, domain-specific ASR solutions for healthcare. With improved data curation, computational resources, and the integration of attention or transformer-based architectures, Ayamra Hospitals can build a scalable and reliable speech recognition system tailored for African clinical settings.

% \section*{References}

% \appendix
% \section*{Appendices}
% Don't know yet

\end{document}