{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AfriSpeech Transformer ASR\n",
                "\n",
                "This notebook implements the end-to-end pipeline for training and evaluating a Transformer-based ASR model (e.g., Whisper or Wav2Vec2) on the AfriSpeech dataset.\n",
                "\n",
                "It covers:\n",
                "1.  Configuration & Setup\n",
                "2.  Data Loading & Preprocessing\n",
                "3.  Model Initialization\n",
                "4.  Training Loop\n",
                "5.  Evaluation & Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Imports & Setup\n",
                "import os\n",
                "import sys\n",
                "import random\n",
                "import numpy as np\n",
                "import torch\n",
                "import pandas as pd\n",
                "import tarfile\n",
                "from torch.utils.data import DataLoader\n",
                "from transformers import get_cosine_schedule_with_warmup\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Check if running in Colab\n",
                "IN_COLAB = 'google.colab' in sys.modules\n",
                "\n",
                "if IN_COLAB:\n",
                "    print(\"Running in Google Colab\")\n",
                "    # Install dependencies\n",
                "    !pip install transformers torchaudio soundfile\n",
                "    \n",
                "    # Mount Drive\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "    \n",
                "    # Add current directory to path (assuming code is uploaded to Colab)\n",
                "    # You might need to adjust this depending on where you upload the code folder\n",
                "    # For example: sys.path.append('/content/drive/MyDrive/path/to/code')\n",
                "    if os.getcwd() not in sys.path:\n",
                "        sys.path.append(os.getcwd())\n",
                "else:\n",
                "    # Add current directory to path to allow importing local modules\n",
                "    if os.getcwd() not in sys.path:\n",
                "        sys.path.append(os.getcwd())\n",
                "\n",
                "# Import local modules\n",
                "# We attempt to find the project root if direct import fails\n",
                "try:\n",
                "    from utils.config import Config\n",
                "except ImportError:\n",
                "    # If running in Colab or a different directory, try to find the 'code' folder\n",
                "    print(\"Direct import failed. Searching for project root...\")\n",
                "    import sys\n",
                "    \n",
                "    # Common paths to check\n",
                "    possible_paths = [\n",
                "        \"/content/drive/MyDrive/AfriSpeech/code\",\n",
                "        \"/content/code\",\n",
                "        os.path.join(os.getcwd(), \"code\")\n",
                "    ]\n",
                "    \n",
                "    found = False\n",
                "    for path in possible_paths:\n",
                "        if os.path.exists(os.path.join(path, \"utils\", \"config.py\")):\n",
                "            print(f\"Found project root at: {path}\")\n",
                "            if path not in sys.path:\n",
                "                sys.path.append(path)\n",
                "            found = True\n",
                "            break\n",
                "    \n",
                "    if not found:\n",
                "        # Fallback: recursively search for utils/config.py\n",
                "        print(\"Searching recursively...\")\n",
                "        for root, dirs, files in os.walk(\"/content\"):\n",
                "            if \"utils\" in dirs and \"config.py\" in os.listdir(os.path.join(root, \"utils\")):\n",
                "                print(f\"Found project root at: {root}\")\n",
                "                if root not in sys.path:\n",
                "                    sys.path.append(root)\n",
                "                found = True\n",
                "                break\n",
                "    \n",
                "    if not found:\n",
                "        raise ImportError(\"Could not find project root containing 'utils/config.py'. Please ensure the code is uploaded and sys.path is correct.\")\n",
                "\n",
                "# Now try importing again (will fail loudly if still not found)\n",
                "from utils.config import Config\n",
                "from data.dataset import AfriSpeechDataset, collate_fn\n",
                "from data.preprocessing import AudioPreprocessor, TextPreprocessor\n",
                "from models.pretrained_asr import PretrainedASRModel\n",
                "from training.trainer import Trainer\n",
                "from training.evaluator import Evaluator\n",
                "from utils.visualization import plot_training_history, save_metric_curves\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "def set_seed(seed: int = 42):\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    torch.cuda.manual_seed_all(seed)\n",
                "    torch.backends.cudnn.deterministic = True\n",
                "    torch.backends.cudnn.benchmark = False\n",
                "\n",
                "set_seed(42)\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1.1 Colab Data Processing\n",
                "# This cell handles data extraction and dataframe creation specifically for the Colab environment\n",
                "\n",
                "if IN_COLAB:\n",
                "    BASE_PATH = \"/content/drive/MyDrive/AfriSpeech\" # 'data' for local\n",
                "    ACCENTS = [\"akan-fante\", \"twi\"]\n",
                "    SPLITS = [\"train\", \"dev\", \"test\"]\n",
                "\n",
                "    def extract_archives(base_path, accents, splits):\n",
                "        for accent in accents:\n",
                "            accent_audio_dir = os.path.join(base_path, \"audio\", accent)\n",
                "\n",
                "            for split in splits:\n",
                "                tar_name = f\"{split}_{accent}_0.tar.gz\"\n",
                "                tar_path = os.path.join(accent_audio_dir, tar_name)\n",
                "\n",
                "                if os.path.exists(tar_path):\n",
                "                    print(f\"extracting {tar_path}...\")\n",
                "                    # Extract to a local directory in Colab for faster access than Drive\n",
                "                    # We'll extract to /content/data/{accent}\n",
                "                    extract_dir = os.path.join(\"/content/data\", accent, split)\n",
                "                    os.makedirs(extract_dir, exist_ok=True)\n",
                "                    \n",
                "                    # Check if already extracted to avoid re-doing it\n",
                "                    if not os.path.exists(os.path.join(extract_dir, \"data\")):\n",
                "                        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
                "                            tar.extractall(path=extract_dir)\n",
                "                        print(f\"extracted to {extract_dir}\")\n",
                "                    else:\n",
                "                        print(f\"Already extracted to {extract_dir}\")\n",
                "                else:\n",
                "                    print(f\"No zip for {accent} {split}, skipped...\")\n",
                "\n",
                "    # Run extraction\n",
                "    # Note: Depending on your Drive structure, you might need to adjust BASE_PATH\n",
                "    # If your data is already extracted or you want to skip this, comment out the next line\n",
                "    # extract_archives(BASE_PATH, ACCENTS, SPLITS)\n",
                "\n",
                "    dfs = {split: [] for split in SPLITS}\n",
                "\n",
                "    def create_dfs(base_path, accents, splits):\n",
                "        for accent in accents:\n",
                "            accent_transcript_dir = os.path.join(base_path, \"transcript\", accent)\n",
                "\n",
                "            for split in splits:\n",
                "                csv_path = os.path.join(accent_transcript_dir, f\"{split}.csv\")\n",
                "                \n",
                "                # IMPORTANT: Update audio_dir to point to where we extracted the files\n",
                "                # If using the extraction logic above:\n",
                "                # audio_dir = os.path.join(\"/content/data\", accent, split, \"data\", \"data\", \"intron\")\n",
                "                \n",
                "                # If using the original Drive path (slower reading):\n",
                "                audio_dir = os.path.join(base_path, \"audio\", accent, f\"{split}_{accent}_0\", \"data\", \"data\", \"intron\")\n",
                "\n",
                "                if not os.path.exists(csv_path):\n",
                "                    print(f\"Didn't find the csv {csv_path}, skipped...\")\n",
                "                    continue\n",
                "\n",
                "                df = pd.read_csv(csv_path)\n",
                "\n",
                "                def make_relative_path(row):\n",
                "                    # Normalize path separators\n",
                "                    audio_path = row[\"audio_paths\"].replace(\"\\\\\", \"/\")\n",
                "                    parts = audio_path.split(\"/\")[-2:]\n",
                "                    # Construct absolute path\n",
                "                    return os.path.join(audio_dir, parts[0], parts[1]).replace(\"\\\\\", \"/\")\n",
                "\n",
                "                # We rename this to 'audio_paths' so the Dataset class uses it directly\n",
                "                df[\"audio_paths\"] = df.apply(make_relative_path, axis=1)\n",
                "                dfs[split].append(df)\n",
                "\n",
                "    create_dfs(BASE_PATH, ACCENTS, SPLITS)\n",
                "\n",
                "    if dfs[\"train\"]:\n",
                "        df_train = pd.concat(dfs[\"train\"], ignore_index=True)\n",
                "        df_val = pd.concat(dfs[\"dev\"], ignore_index=True)\n",
                "        df_test = pd.concat(dfs[\"test\"], ignore_index=True)\n",
                "        \n",
                "        # Save processed CSVs\n",
                "        df_train.to_csv(\"/content/train_processed.csv\", index=False)\n",
                "        df_val.to_csv(\"/content/val_processed.csv\", index=False)\n",
                "        df_test.to_csv(\"/content/test_processed.csv\", index=False)\n",
                "        \n",
                "        print(f\"Processed dataframes saved to /content/\")\n",
                "        print(f\"Train size: {len(df_train)}\")\n",
                "    else:\n",
                "        print(\"No data found. Check BASE_PATH.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Configuration\n",
                "config = Config()\n",
                "\n",
                "# Override config for Colab\n",
                "if IN_COLAB and os.path.exists(\"/content/train_processed.csv\"):\n",
                "    config.data.train_csv = \"/content/train_processed.csv\"\n",
                "    config.data.dev_csv = \"/content/val_processed.csv\"\n",
                "    config.data.test_csv = \"/content/test_processed.csv\"\n",
                "    # Since we constructed absolute paths in the CSV, we set base_dir to root\n",
                "    config.data.audio_base_dir = \"/\"\n",
                "\n",
                "# You can override config values here for experimentation\n",
                "config.training.batch_size = 4\n",
                "config.training.num_epochs = 3\n",
                "config.training.learning_rate = 5e-5\n",
                "config.model.model_name = 'openai/whisper-tiny' # Use tiny for faster testing in notebook\n",
                "\n",
                "print(\"Configuration:\")\n",
                "print(f\"  Model: {config.model.model_name}\")\n",
                "print(f\"  Batch Size: {config.training.batch_size}\")\n",
                "print(f\"  Epochs: {config.training.num_epochs}\")\n",
                "print(f\"  Learning Rate: {config.training.learning_rate}\")\n",
                "print(f\"  Device: {config.device}\")\n",
                "print(f\"  Train CSV: {config.data.train_csv}\")\n",
                "print(f\"  Audio Base: {config.data.audio_base_dir}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Data Preparation\n",
                "\n",
                "# Initialize Audio Preprocessor\n",
                "audio_preprocessor = AudioPreprocessor(\n",
                "    sample_rate=config.data.sample_rate,\n",
                "    n_fft=config.data.n_fft,\n",
                "    hop_length=config.data.hop_length,\n",
                "    n_mels=config.data.n_mels,\n",
                "    f_min=config.data.f_min,\n",
                "    f_max=config.data.f_max\n",
                ")\n",
                "\n",
                "# Initialize Text Preprocessor (will be built later if needed)\n",
                "text_preprocessor = None\n",
                "\n",
                "# We need to initialize the model first to get the processor/tokenizer\n",
                "# This is a bit circular, but necessary for Hugging Face models\n",
                "print(f\"Initializing model wrapper to get processor...\")\n",
                "temp_model = PretrainedASRModel(\n",
                "    model_name=config.model.model_name,\n",
                "    freeze_feature_extractor=config.model.freeze_feature_extractor,\n",
                "    freeze_encoder=config.model.freeze_encoder,\n",
                "    vocab_size=config.model.vocab_size\n",
                ")\n",
                "processor = getattr(temp_model, 'processor', None)\n",
                "model_type = temp_model.model_type\n",
                "del temp_model # Free up memory\n",
                "\n",
                "# Handle Tokenizer/Vocabulary\n",
                "pad_token_id = 0\n",
                "if processor and hasattr(processor, 'tokenizer'):\n",
                "    if processor.tokenizer.pad_token is None:\n",
                "        processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
                "    pad_token_id = processor.tokenizer.pad_token_id\n",
                "else:\n",
                "    print(\"Building custom vocabulary...\")\n",
                "    # Helper to build vocab\n",
                "    def build_vocab_list(csv_path: str) -> list:\n",
                "        import pandas as pd\n",
                "        df = pd.read_csv(csv_path)\n",
                "        return df['transcript'].astype(str).tolist()\n",
                "\n",
                "    train_texts = build_vocab_list(config.data.train_csv)\n",
                "    text_preprocessor = TextPreprocessor(tokenizer_type=config.data.tokenizer_type)\n",
                "    text_preprocessor.build_vocab(train_texts)\n",
                "    pad_token_id = text_preprocessor.vocab.get('<pad>', 0)\n",
                "    print(f\"Vocabulary size: {text_preprocessor.vocab_size}\")\n",
                "\n",
                "# Create Datasets\n",
                "print(\"Loading datasets...\")\n",
                "dataset_kwargs = {\n",
                "    'audio_preprocessor': audio_preprocessor,\n",
                "    'text_preprocessor': text_preprocessor,\n",
                "    'processor': processor,\n",
                "    'model_type': model_type,\n",
                "    'max_audio_length': config.data.max_audio_length,\n",
                "    'min_audio_length': config.data.min_audio_length\n",
                "}\n",
                "\n",
                "train_dataset = AfriSpeechDataset(\n",
                "    csv_path=config.data.train_csv,\n",
                "    audio_base_dir=config.data.audio_base_dir,\n",
                "    use_augmentation=config.data.use_augmentation,\n",
                "    split='train',\n",
                "    **dataset_kwargs\n",
                ")\n",
                "\n",
                "val_dataset = AfriSpeechDataset(\n",
                "    csv_path=config.data.dev_csv,\n",
                "    audio_base_dir=config.data.audio_base_dir,\n",
                "    use_augmentation=False,\n",
                "    split='dev',\n",
                "    **dataset_kwargs\n",
                ")\n",
                "\n",
                "test_dataset = AfriSpeechDataset(\n",
                "    csv_path=config.data.test_csv,\n",
                "    audio_base_dir=config.data.audio_base_dir,\n",
                "    use_augmentation=False,\n",
                "    split='test',\n",
                "    **dataset_kwargs\n",
                ")\n",
                "\n",
                "print(f\"Train samples: {len(train_dataset)}\")\n",
                "print(f\"Val samples: {len(val_dataset)}\")\n",
                "print(f\"Test samples: {len(test_dataset)}\")\n",
                "\n",
                "# Create DataLoaders\n",
                "from functools import partial\n",
                "collate = partial(collate_fn, pad_token_id=pad_token_id if pad_token_id is not None else 0, label_pad_token_id=-100)\n",
                "\n",
                "num_workers = 0 # Set to 0 for Windows/Notebook compatibility usually\n",
                "pin_memory = config.device == 'cuda'\n",
                "\n",
                "train_loader = DataLoader(\n",
                "    train_dataset,\n",
                "    batch_size=config.training.batch_size,\n",
                "    shuffle=True,\n",
                "    collate_fn=collate,\n",
                "    num_workers=num_workers,\n",
                "    pin_memory=pin_memory\n",
                ")\n",
                "\n",
                "val_loader = DataLoader(\n",
                "    val_dataset,\n",
                "    batch_size=config.training.batch_size,\n",
                "    shuffle=False,\n",
                "    collate_fn=collate,\n",
                "    num_workers=num_workers,\n",
                "    pin_memory=pin_memory\n",
                ")\n",
                "\n",
                "test_loader = DataLoader(\n",
                "    test_dataset,\n",
                "    batch_size=config.training.batch_size,\n",
                "    shuffle=False,\n",
                "    collate_fn=collate,\n",
                "    num_workers=num_workers,\n",
                "    pin_memory=pin_memory\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Model Initialization\n",
                "print(f\"Initializing model: {config.model.model_name}...\")\n",
                "model = PretrainedASRModel(\n",
                "    model_name=config.model.model_name,\n",
                "    freeze_feature_extractor=config.model.freeze_feature_extractor,\n",
                "    freeze_encoder=config.model.freeze_encoder,\n",
                "    vocab_size=config.model.vocab_size\n",
                ")\n",
                "\n",
                "print(f\"Model initialized. Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
                "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Training Setup\n",
                "\n",
                "# Optimizer\n",
                "optimizer = torch.optim.AdamW(\n",
                "    model.parameters(),\n",
                "    lr=config.training.learning_rate,\n",
                "    weight_decay=config.training.weight_decay,\n",
                "    betas=(0.9, 0.98)\n",
                ")\n",
                "\n",
                "# Scheduler\n",
                "total_steps = len(train_loader) * config.training.num_epochs\n",
                "warmup_steps = config.training.warmup_steps\n",
                "scheduler = get_cosine_schedule_with_warmup(\n",
                "    optimizer,\n",
                "    num_warmup_steps=warmup_steps,\n",
                "    num_training_steps=total_steps\n",
                ")\n",
                "\n",
                "# Trainer\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    train_loader=train_loader,\n",
                "    val_loader=val_loader,\n",
                "    train_eval_loader=None,\n",
                "    optimizer=optimizer,\n",
                "    scheduler=scheduler,\n",
                "    device=config.device,\n",
                "    output_dir=config.training.output_dir,\n",
                "    save_steps=config.training.save_steps,\n",
                "    eval_steps=config.training.eval_steps,\n",
                "    logging_steps=config.training.logging_steps,\n",
                "    max_grad_norm=config.training.max_grad_norm,\n",
                "    fp16=config.training.fp16,\n",
                "    gradient_accumulation_steps=config.training.gradient_accumulation_steps,\n",
                "    label_smoothing=config.training.label_smoothing\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Training Loop\n",
                "print(\"\\nStarting training...\")\n",
                "for epoch in range(config.training.num_epochs):\n",
                "    print(f\"\\nEpoch {epoch + 1}/{config.training.num_epochs}\")\n",
                "    train_loss = trainer.train_epoch()\n",
                "    print(f\"Train loss: {train_loss:.4f}\")\n",
                "    \n",
                "    # Evaluate on validation set\n",
                "    val_metrics = trainer.evaluate()\n",
                "    print(f\"Val loss: {val_metrics['loss']:.4f}\")\n",
                "    print(f\"Val CER: {val_metrics['cer']:.4f}\")\n",
                "    print(f\"Val WER: {val_metrics['wer']:.4f}\")\n",
                "    print(f\"Val BLEU: {val_metrics['bleu']:.4f}\")\n",
                "    \n",
                "    # Store metrics for visualization\n",
                "    trainer.train_losses.append(train_loss)\n",
                "    trainer.val_losses.append(val_metrics['loss'])\n",
                "    trainer.val_cers.append(val_metrics['cer'])\n",
                "    trainer.val_wers.append(val_metrics['wer'])\n",
                "    trainer.val_bleus.append(val_metrics['bleu'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Evaluation & Visualization\n",
                "\n",
                "# Save final checkpoint\n",
                "trainer.save_checkpoint('final_model.pt')\n",
                "print(\"\\nFinal model saved.\")\n",
                "\n",
                "# Generate visualizations\n",
                "print(\"\\nGenerating visualizations...\")\n",
                "viz_dir = os.path.join(os.getcwd(), 'visualizations_notebook')\n",
                "os.makedirs(viz_dir, exist_ok=True)\n",
                "\n",
                "history_path = os.path.join(viz_dir, 'training_history.png')\n",
                "plot_training_history(\n",
                "    train_losses=trainer.train_losses,\n",
                "    val_losses=trainer.val_losses,\n",
                "    train_eval_losses=None,\n",
                "    train_cers=None,\n",
                "    val_cers=trainer.val_cers,\n",
                "    train_wers=None,\n",
                "    val_wers=trainer.val_wers,\n",
                "    train_bleus=None,\n",
                "    val_bleus=trainer.val_bleus,\n",
                "    save_path=history_path\n",
                ")\n",
                "\n",
                "# Display the plot in the notebook\n",
                "from IPython.display import Image, display\n",
                "display(Image(filename=history_path))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8. Test Set Evaluation\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"EVALUATING ON TEST SET\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "evaluator = Evaluator(trainer.model, device=config.device)\n",
                "test_metrics = evaluator.evaluate(test_loader, decode_fn=None)\n",
                "\n",
                "print(f\"\\nTest Results:\")\n",
                "print(f\"Test CER: {test_metrics['cer']:.4f}\")\n",
                "print(f\"Test WER: {test_metrics['wer']:.4f}\")\n",
                "print(f\"Test BLEU: {test_metrics['bleu']:.4f}\")\n",
                "\n",
                "# Show sample predictions vs ground truth\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"SAMPLE PREDICTIONS vs GROUND TRUTH (First 10 examples)\")\n",
                "print(\"=\"*80)\n",
                "predictions = test_metrics.get('predictions', [])\n",
                "references = test_metrics.get('references', [])\n",
                "\n",
                "for i, (pred, ref) in enumerate(zip(predictions[:10], references[:10])):\n",
                "    print(f\"\\nExample {i+1}:\")\n",
                "    print(f\"GROUND TRUTH: {ref}\")\n",
                "    print(f\"PREDICTION:   {pred}\")\n",
                "    print(\"-\" * 80)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}